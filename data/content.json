{
  "skills": [
    {
      "name": "Linux & Networking",
      "description": "In production, Linux and networking are where most outages first appear: CPU steal on noisy cloud hosts, DNS misconfiguration during cutovers, or firewall rules blocking east-west traffic.",
      "productionScenario": "You are on call for an API fleet behind a load balancer. Error rate spikes after a node replacement. You must validate routing tables, DNS answers, MTU mismatches, and connection tracking saturation before blaming application code.",
      "scaleConsiderations": "At 1-2 servers, manual SSH is acceptable. At 500+ nodes, you need immutable images, standardized OS hardening, and automated diagnostics because one bad kernel/network setting can cascade quickly.",
      "commonMistakes": [
        "Treating DNS as static; not planning TTL strategy before migrations.",
        "Ignoring ephemeral port exhaustion and connection limits under high outbound traffic.",
        "Using ad-hoc SSH changes instead of codified baselines, making incidents harder to reproduce."
      ],
      "costImplications": "Poor network architecture drives cross-AZ/region transfer costs. Over-provisioned bastion/NAT designs and chatty microservices can silently become one of the largest monthly cloud line items.",
      "securityImplications": "Weak SSH key hygiene, broad security groups, and unmanaged sudo access create high blast radius. Use least privilege, host hardening baselines, and centralized audit logging.",
      "incidentExample": "A retail team moved a payments service to new subnets but forgot conditional DNS forwarders in one environment. 30% of checkout calls timed out for 47 minutes until DNS pathing was corrected.",
      "resources": [
        "Linux Journey",
        "The Linux Command Line (book)",
        "Practical Networking"
      ]
    },
    {
      "name": "Version Control & CI/CD",
      "description": "CI/CD is your change management system. In mature teams, pipeline quality directly correlates with deployment safety, lead time, and recovery speed.",
      "productionScenario": "A fintech team deploys multiple times daily. Pipelines run unit/integration/security checks, package artifacts immutably, and gate production with approvals and automated rollback criteria.",
      "scaleConsiderations": "As teams scale, pipeline duration and queue contention become bottlenecks. You need test parallelization, shared build caches, and clear ownership boundaries across repos/services.",
      "commonMistakes": [
        "Running slow monolithic pipelines that engineers bypass under pressure.",
        "No artifact immutability; rebuilding from source at deploy time causes drift.",
        "No release guardrails (health checks/canary analysis), so bad releases spread quickly."
      ],
      "costImplications": "Inefficient pipelines burn compute minutes and engineer time. Flaky tests and repeated reruns can become both a direct CI bill and an indirect productivity tax.",
      "securityImplications": "Leaked CI secrets, over-privileged runners, and unsigned artifacts are common enterprise risks. Adopt OIDC, secret scoping, and provenance/signing where possible.",
      "incidentExample": "A team accidentally deployed debug logging with PII because a policy check was optional in one branch workflow. Incident response required log scrubbing and legal notification.",
      "resources": [
        "Git SCM book",
        "GitHub Actions docs",
        "Jenkins Pipeline docs"
      ]
    },
    {
      "name": "Containers & Orchestration",
      "description": "Containers standardize runtime, but production Kubernetes introduces scheduling, capacity, and networking complexity that must be engineered deliberately.",
      "productionScenario": "Your platform runs 200+ microservices on Kubernetes. During traffic surges, autoscaling, pod disruption budgets, and node headroom determine whether users see degraded performance.",
      "scaleConsiderations": "At scale, noisy neighbors, bin-packing inefficiency, and control-plane limits matter. Right-sizing requests/limits and cluster topology is critical for stability and cost.",
      "commonMistakes": [
        "Setting no CPU/memory limits, allowing one workload to starve others.",
        "Using latest image tags, making rollback and traceability difficult.",
        "Ignoring readiness/startup probes, causing traffic to hit cold or broken pods."
      ],
      "costImplications": "Over-requested resources can double cluster spend. Under-requesting causes throttling/restarts and hidden business cost via poor user experience.",
      "securityImplications": "Running as root, broad service account permissions, and unscanned images are common gaps. Enforce admission policies and image scanning.",
      "incidentExample": "A gaming company saw intermittent 502s after a node pool upgrade because readiness probes were too shallow; pods were marked ready before warming caches.",
      "resources": [
        "Docker docs",
        "Kubernetes docs",
        "Kubernetes the Hard Way"
      ]
    },
    {
      "name": "Cloud & IaC",
      "description": "Infrastructure as Code is essential for repeatability, reviewability, and disaster recovery. In enterprise environments, IaC is also your audit trail.",
      "productionScenario": "A multi-account cloud setup uses Terraform with environment isolation, remote state, and policy checks. Teams promote vetted modules instead of hand-crafting infra per project.",
      "scaleConsiderations": "As the organization grows, state locking, module versioning, and drift detection become mandatory. Without structure, parallel teams create conflicting resources and outages.",
      "commonMistakes": [
        "Single shared state file for too many services causing lock contention and blast radius.",
        "Copy-paste Terraform with hidden differences across environments.",
        "Manual console hotfixes that never get reconciled back to IaC."
      ],
      "costImplications": "Unmanaged cloud growth (orphaned disks, idle clusters, oversized DBs) compounds monthly. IaC with tagging and policy guardrails enables effective FinOps controls.",
      "securityImplications": "Misconfigured IAM, public buckets, and exposed snapshots are recurring issues. Codify preventive controls and scan plans before apply.",
      "incidentExample": "A team restored from backup into production but forgot to rotate credentials in Terraform vars. Legacy credentials remained active and were later abused.",
      "resources": [
        "Terraform docs",
        "AWS Well-Architected",
        "Azure Architecture Center"
      ]
    },
    {
      "name": "Monitoring & Security",
      "description": "Monitoring and security are not separate tracks. In production, detection quality and response speed define customer impact during incidents.",
      "productionScenario": "Your SRE team tracks latency, error rate, traffic, and saturation plus key security signals. Alerting routes by service ownership and severity with clear runbooks.",
      "scaleConsiderations": "At enterprise scale, cardinality explosions in metrics/logs can break observability budgets and systems. You must design telemetry intentionally.",
      "commonMistakes": [
        "Alerting on everything, creating alert fatigue and missed critical pages.",
        "No ownership metadata for services, slowing incident routing.",
        "Collecting high-cardinality labels everywhere, making observability expensive and slow."
      ],
      "costImplications": "Logs and metrics can become top-3 cloud expenses. Sampling, retention tiering, and data lifecycle policies are key to sustainable observability.",
      "securityImplications": "If audit trails are incomplete or mutable, forensic investigations fail. Centralized immutable logging and access monitoring are foundational.",
      "incidentExample": "An enterprise lost 90 minutes during a customer outage because alerts triggered but pager routing was tied to an old team alias; no primary responder was paged.",
      "resources": [
        "Prometheus docs",
        "Grafana docs",
        "OWASP cheat sheets"
      ]
    }
  ],
  "interviewQuestions": [
    {
      "role": "DevOps Engineer",
      "question": "Your canary release increased p95 latency by 25% only in one region. How do you decide rollback vs continue, and what data do you require in 10 minutes?",
      "focus": "Deployment risk and incident decision-making"
    },
    {
      "role": "DevOps Engineer",
      "question": "A CI queue is delaying releases by 90 minutes daily. How would you redesign the pipeline for throughput without reducing test confidence?",
      "focus": "CI/CD at scale"
    },
    {
      "role": "SRE",
      "question": "Your service exceeded error budget for two weeks. How do you negotiate feature freeze, reliability work, and stakeholder communication?",
      "focus": "SLO governance"
    },
    {
      "role": "SRE",
      "question": "During an incident, dashboards disagree with logs and traces. What is your triage order and how do you avoid chasing false signals?",
      "focus": "Incident triage under uncertainty"
    },
    {
      "role": "Platform Engineer",
      "question": "How do you build a self-service platform that enforces secure defaults while keeping developer onboarding under 30 minutes?",
      "focus": "Platform security and developer experience"
    },
    {
      "role": "Cloud Engineer",
      "question": "Your monthly cloud bill jumped 35% after regional expansion. Walk through a cost diagnosis plan and first three remediation actions.",
      "focus": "FinOps and cloud optimization"
    }
  ],
  "toolGuides": [
    {
      "tool": "Docker",
      "whyItExists": "Docker standardizes how an application is packaged and run so developers, CI systems, and production environments behave consistently.",
      "whenToUse": [
        "You need reproducible runtime environments across laptops, CI, and servers.",
        "You are decomposing a system into services with independent release cycles.",
        "You want faster onboarding with predictable local development setup."
      ],
      "whenNotToUse": [
        "Ultra-low latency or hardware-coupled workloads where container overhead/abstractions add unnecessary complexity.",
        "Very small monoliths with infrequent releases where VM/process deployment is already stable.",
        "Teams without operational maturity for image patching/scanning, where containers may increase risk instead of reducing it."
      ],
      "alternatives": [
        "systemd services on VMs",
        "Packer-baked AMIs",
        "Cloud Run/App Service style PaaS"
      ],
      "startupExample": "A 6-engineer startup containers its API + worker + Postgres in docker-compose to keep environments consistent and ship quickly.",
      "enterpriseExample": "A large bank uses hardened base images, signed artifacts, and registry policy controls so every team deploys compliant containers with traceability."
    },
    {
      "tool": "Kubernetes",
      "whyItExists": "Kubernetes automates container scheduling, service discovery, scaling, self-healing, and rollout patterns across clusters.",
      "whenToUse": [
        "You run many services and need standardized deployment, scaling, and health management.",
        "You need multi-team platform abstractions with strong operational guardrails.",
        "You require workload portability across cloud or hybrid environments."
      ],
      "whenNotToUse": [
        "Early-stage products with 1-3 services where platform complexity outweighs benefits.",
        "Teams lacking SRE/platform bandwidth for cluster lifecycle, upgrades, policy, and observability.",
        "Simple batch apps that can run cheaper/simpler on managed serverless platforms."
      ],
      "alternatives": [
        "ECS/Fargate",
        "Nomad",
        "Cloud Run",
        "Heroku-like PaaS"
      ],
      "startupExample": "A startup stays on ECS Fargate until traffic and team size justify a dedicated platform team and deeper orchestration controls.",
      "enterpriseExample": "An enterprise platform team runs multi-cluster Kubernetes with policy-as-code, service mesh, and golden paths for 200+ services."
    },
    {
      "tool": "Terraform",
      "whyItExists": "Terraform provides declarative, versioned infrastructure management so changes are reviewable, repeatable, and auditable.",
      "whenToUse": [
        "You manage cloud resources across environments/accounts and need consistency.",
        "You want infrastructure changes reviewed through pull requests with rollback history.",
        "You need reusable modules for standardized network, IAM, and platform patterns."
      ],
      "whenNotToUse": [
        "One-off experimental sandboxes where setup speed matters more than lifecycle governance.",
        "Very dynamic app-level config that is better managed by Kubernetes operators or app tooling.",
        "Teams unable to enforce state hygiene and module discipline, causing IaC drift and lock contention."
      ],
      "alternatives": [
        "CloudFormation",
        "Pulumi",
        "Crossplane",
        "Native cloud console (short-term only)"
      ],
      "startupExample": "A startup starts with a small Terraform root module for VPC, database, and compute, then modularizes as environments grow.",
      "enterpriseExample": "A global enterprise enforces approved Terraform modules and policy checks in CI to prevent insecure or non-compliant infrastructure changes."
    },
    {
      "tool": "CI/CD",
      "whyItExists": "CI/CD reduces deployment risk and lead time by automating build, test, security checks, release orchestration, and rollback workflows.",
      "whenToUse": [
        "You deploy frequently and need reliable repeatable delivery.",
        "You want quality/security gates before changes reach production.",
        "You need traceability from commit to artifact to deployment."
      ],
      "whenNotToUse": [
        "Teams with no test strategy; automation without trustable tests just accelerates defects.",
        "Highly regulated manual release checkpoints that are not yet encoded as pipeline controls.",
        "Tiny internal tools updated quarterly where manual release is lower overhead."
      ],
      "alternatives": [
        "Manual release checklist",
        "GitOps controllers (Argo CD/Flux)",
        "Vendor release pipelines"
      ],
      "startupExample": "A startup uses GitHub Actions with basic test + deploy jobs and gradually adds canary checks as customer impact grows.",
      "enterpriseExample": "An enterprise runs layered pipelines with security scans, separation-of-duties approvals, staged rollouts, and automated rollback triggers."
    }
  ],
  "labs": [
    {
      "topic": "Docker",
      "objective": "Containerize a sample API, push image to Amazon ECR, and run it in Amazon EKS with health checks.",
      "architectureDiagramDescription": "Developer laptop builds Docker image -> pushes to Amazon ECR -> Kubernetes Deployment in EKS pulls image -> Service exposes app internally -> Ingress/LoadBalancer exposes endpoint.",
      "stepByStepCommands": [
        "aws sts get-caller-identity",
        "aws ecr create-repository --repository-name devops-lab-api --region us-east-1",
        "aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com",
        "cat > Dockerfile <<'EOF'\nFROM public.ecr.aws/docker/library/nginx:1.27-alpine\nCOPY ./site /usr/share/nginx/html\nEXPOSE 80\nEOF",
        "mkdir -p site && echo '<h1>docker-lab-ok</h1>' > site/index.html",
        "docker build -t devops-lab-api:1.0 .",
        "docker tag devops-lab-api:1.0 <ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com/devops-lab-api:1.0",
        "docker push <ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com/devops-lab-api:1.0",
        "kubectl create ns devops-labs",
        "kubectl -n devops-labs create deployment docker-lab --image=<ACCOUNT_ID>.dkr.ecr.us-east-1.amazonaws.com/devops-lab-api:1.0",
        "kubectl -n devops-labs expose deployment docker-lab --type=LoadBalancer --port=80 --target-port=80",
        "kubectl -n devops-labs get pods,svc"
      ],
      "expectedOutput": [
        "ECR repository URI is returned.",
        "docker push shows image layers uploaded.",
        "kubectl get pods shows docker-lab pod Running and Ready 1/1.",
        "Service gets EXTERNAL-IP (or hostname) and curl returns 'docker-lab-ok'."
      ],
      "commonFailureCases": [
        "ImagePullBackOff due to missing ECR auth/IAM permissions for worker nodes.",
        "CrashLoopBackOff from bad Dockerfile entrypoint or missing files.",
        "LoadBalancer pending because cluster/subnet lacks cloud LB integration.",
        "Architecture mismatch (built amd64 image, running arm64 nodes)."
      ],
      "cleanupSteps": [
        "kubectl -n devops-labs delete svc docker-lab",
        "kubectl -n devops-labs delete deploy docker-lab",
        "kubectl delete ns devops-labs",
        "aws ecr batch-delete-image --repository-name devops-lab-api --image-ids imageTag=1.0 --region us-east-1",
        "aws ecr delete-repository --repository-name devops-lab-api --force --region us-east-1"
      ]
    },
    {
      "topic": "Kubernetes",
      "objective": "Deploy a highly available app on EKS with rolling updates, HPA, and PodDisruptionBudget.",
      "architectureDiagramDescription": "EKS control plane manages Deployments -> ReplicaSet schedules pods across worker nodes in multiple AZs -> Service routes traffic -> HPA scales replicas based on CPU.",
      "stepByStepCommands": [
        "kubectl create ns k8s-lab",
        "kubectl -n k8s-lab create deployment web --image=public.ecr.aws/nginx/nginx:stable",
        "kubectl -n k8s-lab scale deployment web --replicas=3",
        "kubectl -n k8s-lab expose deployment web --port=80 --type=ClusterIP",
        "kubectl -n k8s-lab set resources deployment web --requests=cpu=100m,memory=128Mi --limits=cpu=300m,memory=256Mi",
        "kubectl -n k8s-lab autoscale deployment web --cpu-percent=60 --min=3 --max=10",
        "cat > pdb.yaml <<'EOF'\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: web-pdb\n  namespace: k8s-lab\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: web\nEOF",
        "kubectl apply -f pdb.yaml",
        "kubectl -n k8s-lab rollout status deploy/web",
        "kubectl -n k8s-lab set image deploy/web nginx=public.ecr.aws/nginx/nginx:1.27-perl",
        "kubectl -n k8s-lab rollout status deploy/web",
        "kubectl -n k8s-lab get deploy,hpa,pdb,pods"
      ],
      "expectedOutput": [
        "Deployment maintains at least 3 running pods.",
        "HPA object is created with min/max values.",
        "Rollout completes with zero downtime (no full outage of endpoints).",
        "PDB prevents voluntary disruptions from dropping available pods below 2."
      ],
      "commonFailureCases": [
        "HPA stuck with '<unknown>' metrics because metrics-server is missing.",
        "Rollout stalls due to invalid image tag.",
        "PDB appears ineffective when selector labels do not match pods.",
        "Resource requests too low causing CPU throttling and latency spikes."
      ],
      "cleanupSteps": [
        "kubectl delete ns k8s-lab",
        "rm -f pdb.yaml"
      ]
    },
    {
      "topic": "Terraform",
      "objective": "Provision EKS-ready AWS networking (VPC + subnets + NAT) using Terraform with remote state.",
      "architectureDiagramDescription": "Terraform CLI -> S3 backend + DynamoDB lock table -> AWS VPC module creates public/private subnets across AZs -> NAT gateway for egress -> outputs consumed by EKS module.",
      "stepByStepCommands": [
        "aws s3 mb s3://<UNIQUE_TF_STATE_BUCKET> --region us-east-1",
        "aws dynamodb create-table --table-name tf-locks --attribute-definitions AttributeName=LockID,AttributeType=S --key-schema AttributeName=LockID,KeyType=HASH --billing-mode PAY_PER_REQUEST --region us-east-1",
        "cat > providers.tf <<'EOF'\nterraform {\n  required_version = \">= 1.6.0\"\n  backend \"s3\" {\n    bucket         = \"<UNIQUE_TF_STATE_BUCKET>\"\n    key            = \"labs/network/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"tf-locks\"\n    encrypt        = true\n  }\n}\nprovider \"aws\" {\n  region = \"us-east-1\"\n}\nEOF",
        "cat > main.tf <<'EOF'\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"5.8.1\"\n  name = \"labs-vpc\"\n  cidr = \"10.50.0.0/16\"\n  azs  = [\"us-east-1a\",\"us-east-1b\"]\n  private_subnets = [\"10.50.1.0/24\",\"10.50.2.0/24\"]\n  public_subnets  = [\"10.50.101.0/24\",\"10.50.102.0/24\"]\n  enable_nat_gateway = true\n  single_nat_gateway = true\n}\noutput \"vpc_id\" { value = module.vpc.vpc_id }\nEOF",
        "terraform init",
        "terraform fmt -recursive",
        "terraform validate",
        "terraform plan -out tfplan",
        "terraform apply tfplan",
        "terraform output vpc_id"
      ],
      "expectedOutput": [
        "Terraform initializes S3 backend and acquires DynamoDB lock.",
        "Plan shows VPC, subnets, route tables, IGW, and NAT resources.",
        "Apply completes with output vpc_id.",
        "State file is stored in S3, not local disk."
      ],
      "commonFailureCases": [
        "State lock errors from stale DynamoDB lock items.",
        "NAT gateway creation fails due to Elastic IP quota limits.",
        "Overlapping CIDR ranges with existing VPCs.",
        "Manual console edits causing drift and unexpected future plans."
      ],
      "cleanupSteps": [
        "terraform destroy -auto-approve",
        "aws dynamodb delete-table --table-name tf-locks --region us-east-1",
        "aws s3 rb s3://<UNIQUE_TF_STATE_BUCKET> --force"
      ]
    },
    {
      "topic": "CI/CD",
      "objective": "Build a GitHub Actions pipeline that tests, builds Docker image, pushes to ECR, and deploys to EKS using kubectl.",
      "architectureDiagramDescription": "Git push triggers GitHub Actions -> workflow runs tests -> builds image -> authenticates to ECR via OIDC/IAM role -> pushes image -> updates Kubernetes Deployment in EKS namespace.",
      "stepByStepCommands": [
        "aws iam create-open-id-connect-provider --url https://token.actions.githubusercontent.com --thumbprint-list 6938fd4d98bab03faadb97b34396831e3780aea1 --client-id-list sts.amazonaws.com",
        "aws iam create-role --role-name github-actions-ecr-eks --assume-role-policy-document file://trust-policy.json",
        "aws iam attach-role-policy --role-name github-actions-ecr-eks --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPowerUser",
        "aws iam attach-role-policy --role-name github-actions-ecr-eks --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy",
        "mkdir -p .github/workflows",
        "cat > .github/workflows/lab-cicd.yml <<'EOF'\nname: lab-cicd\non: [push]\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: read\n    steps:\n      - uses: actions/checkout@v4\n      - uses: aws-actions/configure-aws-credentials@v4\n        with:\n          role-to-assume: arn:aws:iam::<ACCOUNT_ID>:role/github-actions-ecr-eks\n          aws-region: us-east-1\n      - uses: aws-actions/amazon-ecr-login@v2\n      - run: docker build -t $ECR_REPO:$GITHUB_SHA .\n      - run: docker push $ECR_REPO:$GITHUB_SHA\n      - run: kubectl -n devops-labs set image deployment/docker-lab docker-lab=$ECR_REPO:$GITHUB_SHA\nEOF",
        "git add .github/workflows/lab-cicd.yml && git commit -m 'add lab cicd workflow'",
        "git push",
        "kubectl -n devops-labs rollout status deployment/docker-lab"
      ],
      "expectedOutput": [
        "GitHub Actions run shows successful credential federation (OIDC).",
        "Docker image is pushed to ECR with commit SHA tag.",
        "Deployment image updates and rollout completes successfully.",
        "Application endpoint serves new version."
      ],
      "commonFailureCases": [
        "OIDC trust policy audience/subject mismatch prevents role assumption.",
        "ECR push denied due to insufficient IAM permissions.",
        "kubectl unauthorized because kubeconfig context is not configured in workflow.",
        "Rollout fails due to missing readiness probes causing premature traffic routing."
      ],
      "cleanupSteps": [
        "kubectl -n devops-labs rollout undo deployment/docker-lab",
        "aws iam detach-role-policy --role-name github-actions-ecr-eks --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryPowerUser",
        "aws iam detach-role-policy --role-name github-actions-ecr-eks --policy-arn arn:aws:iam::aws:policy/AmazonEKSClusterPolicy",
        "aws iam delete-role --role-name github-actions-ecr-eks",
        "rm -f .github/workflows/lab-cicd.yml"
      ]
    },
    {
      "topic": "Docker",
      "labType": "Break & Fix",
      "objective": "Diagnose and fix a containerized service that starts but fails health checks due to bad entrypoint and port/probe mismatch.",
      "architectureDiagramDescription": "Broken Docker image in ECR -> EKS Deployment pulls image -> Pod restarts and stays unready -> Service has no healthy endpoints.",
      "intentionalMisconfiguration": [
        "Dockerfile/command points to wrong binary path.",
        "App listens on 8080 while probe/service expects 80.",
        "Readiness endpoint configured as /healthz but app serves /health."
      ],
      "stepByStepCommands": [
        "kubectl create ns breakfix-labs",
        "kubectl -n breakfix-labs apply -f broken-docker-deploy.yaml",
        "kubectl -n breakfix-labs get pods -w",
        "kubectl -n breakfix-labs logs deploy/docker-breakfix --tail=100",
        "kubectl -n breakfix-labs describe pod -l app=docker-breakfix",
        "kubectl -n breakfix-labs get events --sort-by=.lastTimestamp | tail -n 30",
        "kubectl -n breakfix-labs top pod",
        "kubectl -n breakfix-labs edit deploy docker-breakfix",
        "kubectl -n breakfix-labs rollout status deploy/docker-breakfix",
        "kubectl -n breakfix-labs port-forward deploy/docker-breakfix 8080:8080",
        "curl -i http://127.0.0.1:8080/health"
      ],
      "expectedOutput": [
        "Before fix: CrashLoopBackOff and readiness probe failures.",
        "Logs/events show executable path and probe/port mismatch.",
        "After fix: pod Running 1/1 and health endpoint returns 200."
      ],
      "commonFailureCases": [
        "Fixing command but not probe path keeps pod unready.",
        "Skipping describe/events causes wrong root-cause assumptions.",
        "kubectl top fails when metrics-server is not installed."
      ],
      "sreTroubleshootingWorkflow": [
        "Start with user symptom and blast radius.",
        "Correlate pod status + restart reason + events timeline.",
        "Use logs for app truth and events for platform truth.",
        "Validate entrypoint, ports, probes, and env assumptions.",
        "Roll out minimal safe fix and verify health + traffic.",
        "Document RCA and add CI guardrails to prevent recurrence."
      ],
      "solutionExplanation": "Real SRE triage combines logs, events, and metrics rather than changing config blindly. This issue is resolved by restoring runtime contract alignment (command, port, probe), validating rollout, and adding pre-deploy container/probe checks.",
      "cleanupSteps": [
        "kubectl delete ns breakfix-labs"
      ]
    },
    {
      "topic": "Kubernetes",
      "labType": "Break & Fix",
      "objective": "Debug and recover a failing workload where routing and readiness are broken by selector/probe misconfiguration.",
      "architectureDiagramDescription": "Deployment labels do not match Service selectors -> no endpoints; readiness probe fails; HPA lacks usable metrics.",
      "intentionalMisconfiguration": [
        "Service selector uses app=payments-api while pods are labeled app=payment-api.",
        "Readiness probe path set to /readyz though app only serves /health.",
        "HPA configured but metrics pipeline unavailable."
      ],
      "stepByStepCommands": [
        "kubectl create ns breakfix-k8s",
        "kubectl -n breakfix-k8s apply -f broken-k8s-stack.yaml",
        "kubectl -n breakfix-k8s get deploy,svc,pods,hpa",
        "kubectl -n breakfix-k8s get endpoints payments-api -o yaml",
        "kubectl -n breakfix-k8s describe svc payments-api",
        "kubectl -n breakfix-k8s describe pod -l app=payment-api",
        "kubectl -n breakfix-k8s logs deploy/payment-api --tail=100",
        "kubectl -n breakfix-k8s get events --sort-by=.lastTimestamp | tail -n 40",
        "kubectl -n breakfix-k8s top pod",
        "kubectl -n breakfix-k8s patch svc payments-api -p '{\"spec\":{\"selector\":{\"app\":\"payment-api\"}}}'",
        "kubectl -n breakfix-k8s edit deploy payment-api",
        "kubectl -n breakfix-k8s rollout restart deploy/payment-api",
        "kubectl -n breakfix-k8s rollout status deploy/payment-api",
        "kubectl -n breakfix-k8s get endpoints payments-api"
      ],
      "expectedOutput": [
        "Before fix: empty endpoints and readiness failures in events.",
        "HPA shows <unknown> when metrics pipeline is broken.",
        "After fix: endpoints populated and deployment Available."
      ],
      "commonFailureCases": [
        "Fixing only Service or only Deployment labels (must match both).",
        "Restarting rollout before fixing readiness probe path.",
        "Treating HPA metric errors as app defects instead of observability defects."
      ],
      "sreTroubleshootingWorkflow": [
        "Determine failure domain: routing, app, or scaling.",
        "Trace request path Service -> Endpoints -> Pods.",
        "Use events to build incident timeline.",
        "Use logs for app confirmation and probe compatibility.",
        "Separate metrics-pipeline failures from app failures.",
        "Verify recovery against SLO-facing indicators."
      ],
      "solutionExplanation": "SRE troubleshooting is layered: restore routing first, then readiness, then scaling confidence. Selector mismatch caused no endpoints, probe path caused unready pods, and HPA unknown metrics required separate platform follow-up.",
      "cleanupSteps": [
        "kubectl delete ns breakfix-k8s"
      ]
    }
  ],
  "learningPaths": [
    {
      "name": "Beginner DevOps Engineer",
      "requiredLessons": [
        "Linux & Networking fundamentals",
        "Git workflows and CI/CD basics",
        "Docker image and container lifecycle",
        "Cloud and Infrastructure as Code basics",
        "Monitoring, logging, and incident fundamentals"
      ],
      "requiredLabs": [
        "Docker Lab (ECR + EKS deployment)",
        "Kubernetes Lab (Deployment, HPA, PDB)",
        "Terraform Lab (VPC + remote state)",
        "CI/CD Lab (GitHub Actions to EKS)"
      ],
      "skillsGained": [
        "Build and deploy containerized services",
        "Understand CI/CD release flow end-to-end",
        "Apply core Kubernetes operational practices",
        "Use Terraform for reproducible infrastructure",
        "Troubleshoot basic production incidents"
      ]
    },
    {
      "name": "Kubernetes Administrator",
      "requiredLessons": [
        "Kubernetes control plane and worker model",
        "Workload management (Deployments, Services, Ingress)",
        "Probes, autoscaling, and disruption budgets",
        "Cluster observability and troubleshooting",
        "Kubernetes security and RBAC fundamentals"
      ],
      "requiredLabs": [
        "Kubernetes Lab (rolling updates + HPA + PDB)",
        "Kubernetes Break & Fix Lab (selector/probe mismatch)",
        "Docker Break & Fix Lab (runtime/probe diagnostics)"
      ],
      "skillsGained": [
        "Operate Kubernetes workloads safely",
        "Debug routing/readiness failures using logs/events/metrics",
        "Apply scaling and resilience controls",
        "Implement secure cluster access patterns",
        "Run production-style triage and recovery"
      ]
    },
    {
      "name": "DevOps Engineer (AWS)",
      "requiredLessons": [
        "AWS networking and IAM foundations",
        "Terraform on AWS with state locking",
        "ECR/EKS container delivery patterns",
        "CI/CD with OIDC and GitHub Actions",
        "Cost, reliability, and security guardrails"
      ],
      "requiredLabs": [
        "Terraform Lab (AWS VPC + S3 backend + DynamoDB locks)",
        "Docker Lab (build/push to ECR and deploy to EKS)",
        "CI/CD Lab (OIDC + ECR + EKS rollout)",
        "Kubernetes Break & Fix Lab"
      ],
      "skillsGained": [
        "Design AWS-native DevOps workflows",
        "Deploy secure, automated release pipelines",
        "Manage infra lifecycle using Terraform",
        "Operate EKS workloads with production checks",
        "Balance delivery speed with cloud cost controls"
      ]
    },
    {
      "name": "SRE Path",
      "requiredLessons": [
        "SLO/SLI and error budget governance",
        "Incident management and communication",
        "Observability design (metrics, logs, traces)",
        "Reliability patterns in Kubernetes and CI/CD",
        "Post-incident RCA and prevention engineering"
      ],
      "requiredLabs": [
        "Kubernetes Break & Fix Lab",
        "Docker Break & Fix Lab",
        "Kubernetes Lab (resilience controls)",
        "CI/CD Lab (safe rollout and rollback flow)"
      ],
      "skillsGained": [
        "Lead incident triage and resolution under pressure",
        "Use telemetry to isolate failure domains quickly",
        "Improve reliability via runbooks and guardrails",
        "Enforce safe release strategies with rollback planning",
        "Translate reliability risk to business impact"
      ]
    }
  ]
}
